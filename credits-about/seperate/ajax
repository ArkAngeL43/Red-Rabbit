

==============================================================================
MODULE 3 -> CHAPTER 2 ( ajax spiders)


in this script i have three types of ajax spiders ( all written in golang )

first lets go over the command and beif description 

|-> ajax-wo | Ajax spider without whois for every url scanned                           
|-> ajax-NK | Ajax spider NUCLEAR aka a over powered crawler                           
|-> ajax-wu | Ajax spider with whois for every url scanned  

what is the difference between a ajax spider without whois and a ajax with WHOIS 

first lets go over what these spiders do, spiders are just simple plain web crawlers asides these are not 

the average web crawler they work a little bit differently- well the first option we have here ajax-wo

is the more simple plain ajax spider that doesnt dig every single url it finds and rather does a basic 

crawl and investigation on the URL, get the base request headers, find the server, doamin, and IP of

every url it found and write it to a file called ip-out, out.txt etc a basic output will look like this

````


[>] Script Started At ->  2021-12-21 14:45:20.431218871 -0500 EST m=+0.378728587
[*]Server IPA ->  [2607:f8b0:4008:809::2004 142.250.64.132]
[*] Skipping....No URLs found in Copy

[*] Crawling URL >>  http://www.google.com
 ─────────────────────────Server Response─────────────────────────────
[*] Response Status  ->  200 OK
[*] Date Of Request  ->  Wed, 22 Dec 2021 00:45:16 GMT
[*] Content-Encoding ->  
[*] Content-Type     ->  text/html; charset=ISO-8859-1
[*] Connected-Server ->  gws
[*] X-Frame-Options  ->  SAMEORIGIN
[*] Scheme        --->  https
[*] Hostname      --->  www.google.com
[*] Path in URL   --->  
[*] Query Strings --->  
[*] Fragments     --->  
[*]-> Cache-Control -> [private, max-age=0]
[*]-> Content-Type -> [text/html; charset=ISO-8859-1]
[*]-> P3p -> [CP="This is not a P3P policy! See g.co/p3phelp for more info."]
[*]-> Server -> [gws]
[*]-> X-Xss-Protection -> [0]
[*]-> X-Frame-Options -> [SAMEORIGIN]
[*]-> Date -> [Wed, 22 Dec 2021 00:45:16 GMT]
[*]-> Set-Cookie -> [1P_JAR=2021-12-22-00; expires=Fri, 21-Jan-2022 00:45:16 GMT; path=/; domain=.google.com; Secure NID=511=VhWDumG-Qah28PC4EBZOAxrU8Sl7yfNXlE5tD_NUZ-4RIpHLJTBaHsrbspiHuVw-0lXpHY34r_eAdXZkXOiQckO4atJ9tjkMdEx97hc3VUkqOH3cr406yG_5d3FJ_FhA04398KgVoVVyES9A09Vw_mWbQwTGnBfSOBYxMlf-JPY; expires=Thu, 23-Jun-2022 00:45:16 GMT; path=/; domain=.google.com; HttpOnly]
[*]-> Expires -> [-1]
──────────────────────────────────────────────────────
[*] URL Found ->  http://www.google.com
34
[*] Domain Name ->  google.com
[*] Domain IPA  ->  [2607:f8b0:4008:801::200e 142.250.189.142]
56
[*] Connected-Server ->  gws
[*] Response Status  ->  200 OK
 ──────────────────────────────────────────────────────
````

as you can see it found the URl, response, request headers, response headers, server status, domain, and even out put 

the amount of bytes of data written to the file


lets now look at the most over powered command of them all and that is ajax-NK i will have everyone note this is A VERY 

UNSTABLE AND ILLEGAL SCRIPT, this goes above and beyond in terms of testing. the features it has are 

getting the following information 
=================================

server, server name, server date, request headers, whois, IP6, IP4, Every URl, The HTML files, download every HTML file,

admin URLs, SQL urls, the title, the domain name, the domain dig, the domain response, the tracker, and if the URl comes 

across as dangerous it attempt to redirect it through the loopback and make a non private connection to the host blocking the 

host from accessing your data or PC or even browser. 

lets go over what it does upon entering data 



first we need to look at the required input red rabbit gives us for the basic parsing

when we first choose the option ajax-nk, we are prompted to the following required inputs 

HTTPS-URL >
DOMAIN    >
BASE-HTTP >

we know the basic strings, https url, domain etc but what is the base http?

the bas http is just another term for a http url like this http://www.google.com is a base http url

when we enter that data we are prompted for a question

```
for every url the spider finds, would you like to download the HTML page?
```

*************** WARNING ************************
* this WILL TAKE UP ALOT OF MEMORY AND SPACE   *
************************************************
if you select yes then it will continue onto scraping 

lets now look at an example output used 


first we see the most common given that being 

```
[>] Script Started At ->  2021-12-21 17:35:42.015359568 -0500 EST m=+0.317608974
[*]Server IPA ->  [2607:f8b0:4008:813::2004 172.217.15.196]
[*] Skipping....No URLs found in Copy

```

this gets the target IP, and sees if you have any URL's in your notepad/copy pad

then we get the stat 

```

[*] Crawling URL >>  http://www.google.com
 	─────────────────────────Server Response─────────────────────────────
[*] Response Status  ->  200 OK
[*] Date Of Request  ->  Wed, 22 Dec 2021 03:35:38 GMT
[*] Content-Encoding ->  
[*] Content-Type     ->  text/html; charset=ISO-8859-1
[*] Connected-Server ->  gws
[*] X-Frame-Options  ->  SAMEORIGIN
[*] Scheme        --->  https
[*] Hostname      --->  www.google.com
[*] Path in URL   --->  
[*] Query Strings --->  
[*] Fragments     --->  
	[  INFO  ]  P3p	 -> [CP="This is not a P3P policy! See g.co/p3phelp for more info."]
	[  INFO  ]  Server	 -> [gws]
	[  INFO  ]  X-Xss-Protection	 -> [0]
	[  INFO  ]  Set-Cookie	 -> [1P_JAR=2021-12-22-03; expires=Fri, 21-Jan-2022 03:35:38 GMT; path=/; domain=.google.com; Secure NID=511=gGgqHkU6alL9mvtwZwFonTuFs7J4UHx2L3oPsCahENFR2wvMC154y2NyruMeHrd12prFIAJY1Pv20NTVfZbOP07K2MQxz6TbAWz-RfhPo4NAqefrNNui4fPMRX9FLte0OVFiMkl4O9C_Qyt55msFVAgmFPeVoValuXO3QiW8VT4; expires=Thu, 23-Jun-2022 03:35:38 GMT; path=/; domain=.google.com; HttpOnly]
	[  INFO  ]  Date	 -> [Wed, 22 Dec 2021 03:35:38 GMT]
	[  INFO  ]  Expires	 -> [-1]
	[  INFO  ]  Content-Type	 -> [text/html; charset=ISO-8859-1]
	[  INFO  ]  Cache-Control	 -> [private, max-age=0]
	[  INFO  ]  X-Frame-Options	 -> [SAMEORIGIN]

```

the top in frgaments and what not parses extreme urls an exmaple is 

https://www.google.com/index.php/id=1./as/diosrgdruoghedrogoedurgweffij4803895u34ID4wr3rID=13344FopR3-4-4-4-etctetctetctdt

which will give everything on that basic URL in itself


then we go to the base targeting, it will attempt to search for over 400+ admin panel URL's and parse it 


an example as the folloqwing output will be shown 


```

 	[  INFO  ]  ->  https://www.googleeeeeee.com/modelsearch/login.asp   IS NOT VULNERABLE
 	[  INFO  ]  ->  https://www.googleeeeeee.com/moderator.asp   IS NOT VULNERABLE
 	[  INFO  ]  ->  https://www.googleeeeeee.com/moderator/login.asp   IS VULNERABLE
 	[  INFO  ]  ->  https://www.googleeeeeee.com/administrator/login.asp   IS NOT VULNERABLE
 	[  INFO  ]  ->  https://www.googleeeeeee.com/moderator/admin.asp   IS VULNERABLE
 	[  INFO  ]  ->  https://www.googleeeeeee.com/controlpanel.asp   IS NOT VULNERABLE
 	[  INFO  ]  ->  https://www.googleeeeeee.com/user.asp   IS NOT VULNERABLE
 	[  INFO  ]  ->  https://www.googleeeeeee.com/admincontrol.asp   IS NOT VULNERABLE
 	[  INFO  ]  ->  https://www.googleeeeeee.com/adminpanel.asp   IS NOT VULNERABLE

# NOTE: GOOGLE WAS NOT A MAIN CRAWL AND OUTPUT THESE ARE EXAMPLES ONLY #google please dont sue me

```

once done it will continue to scan for urls, for every URL found it will output the following


```
	[ INFO ]  FOUND DOMAIN NAME => example.com
	[*] Scan Results for   ├ example.com (0.0.0.0.)
	[+]			┡ 80	http
	[+]			┡ 443	https
	[ INFO ]  FOUND DOMAIN IP => [0000:0000:0000:0000:0000. 0.0.0.0.0]
	[ INFO ]  FOUND SERVER => gws
	[ INFO ]  RESPO STATUS => 200OK	[ INFO ]  FOUND URL => http://www.google.example.com.net.edu
    [  INFO  ] Server HAS PASSED ALL INJECTIONS, NOT VULNERABLE
 	[  INFO  ] Server HAS PASSED ALL INJECTIONS, NOT VULNERABLE
 	[  INFO  ] Server HAS PASSED ALL INJECTIONS, NOT VULNERABLE
 	[  INFO  ] Server HAS PASSED ALL INJECTIONS, NOT VULNERABLE
 	[  INFO  ] Server HAS PASSED ALL INJECTIONS, NOT VULNERABLE
 	[  INFO  ] Server HAS PASSED ALL INJECTIONS, NOT VULNERABLE
 	[  INFO  ] Server HAS PASSED ALL INJECTIONS, NOT VULNERABLE
 	[  INFO  ] Server HAS PASSED ALL INJECTIONS, NOT VULNERABLE
 	[  INFO  ] Server HAS PASSED ALL INJECTIONS, NOT VULNERABLE
 	[  INFO  ] Server HAS PASSED ALL INJECTIONS, NOT VULNERABLE
 	[  INFO  ] Server HAS PASSED ALL INJECTIONS, NOT VULNERABLE
 	[  INFO  ] Server HAS PASSED ALL INJECTIONS, NOT VULNERABLE
 	[  INFO  ] Server HAS PASSED ALL INJECTIONS, NOT VULNERABLE
 	[  INFO  ] Server HAS PASSED ALL INJECTIONS, NOT VULNERABLE
 	[  INFO  ] Server HAS PASSED ALL INJECTIONS, NOT VULNERABLE
 	[  INFO  ] Server HAS PASSED ALL INJECTIONS, NOT VULNERABLE
 	[  INFO  ] Server HAS PASSED ALL INJECTIONS, NOT VULNERABLE
 	[  INFO  ] Server HAS PASSED ALL INJECTIONS, NOT VULNERABLE
 	[  INFO  ] Server HAS PASSED ALL INJECTIONS, NOT VULNERABLE
 	[  INFO  ] Server HAS PASSED ALL INJECTIONS, NOT VULNERABLE
	[  INFO  ]  Cache-Control	 -> [private, max-age=0]
	[  INFO  ]  X-Xss-Protection	 -> [0]
	[  INFO  ]  Alt-Svc	 -> [h3=":443"; ma=2592000,h3-29=":443"; ma=2592000,h3-Q050=":443"; ma=2592000,h3-Q046=":443"; ma=2592000,h3-Q043=":443"; ma=2592000,quic=":443"; ma=2592000; v="46,43"]
	[  INFO  ]  Date	 -> [Wed, 22 Dec 2021 03:35:57 GMT]
	[  INFO  ]  Expires	 -> [-1]
	[  INFO  ]  Server	 -> [gws]
	[  INFO  ]  X-Frame-Options	 -> [SAMEORIGIN]
	[  INFO  ]  Set-Cookie	 -> [1P_JAR=2021-12-22-03; expires=Fri, 21-Jan-2022 03:35:57 GMT; path=/; domain=.google.com; Secure NID=511=n0swf6VjIix9VCC7a7JAYaC-0s1ylNi4rbYwK5IK7f3XGf0zoPDoyV6LiK8YdZ1Hxdlcq8MqQGR0Sk_Zpoer1LWymbYZc8I836kBnvpE75f4kWjExNnqWCLJdElygio35kSGNqV-AEu7_dybyUFcbpvlbmV4HPzL0wIPOApjwsE; expires=Thu, 23-Jun-2022 03:35:57 GMT; path=/; domain=.google.com; HttpOnly]
	[  INFO  ]  Content-Type	 -> [text/html; charset=UTF-8]
	[  INFO  ]  P3p	 -> [CP="This is not a P3P policy! See g.co/p3phelp for more info."]
Title of the page:   example domain  
64

```

lets walk through this output, first you have the found domain name this is the parsed domain 

it will parse the domain into my own module i wrote for port scanning and port scan the domain 

then get the IP6-IP4, server, host, and status following that attempt to SQL inject the host 20+ times

and finally get the host information or the response/GET headers of the URL



if you added the option yes of downloading all the html files in range of every url you might see something 

like this 

```
AEu7_dybyUFcbpvlbmV4HPzL0wIPOApjwsE.html
```

this is the HTMl of EACH URL's HTML file you downloaded

it works like this 


go code 

````
var chars = "abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ1234567890-"

func shortID(length int) string {
	ll := len(chars)
	b := make([]byte, length)
	rand.Read(b)
	for i := 0; i < length; i++ {
		b[i] = chars[int(b[i])%ll]
	}
	return string(b)
}

````

this generates a random string, and applys it to a definition that is called in the go code under a for

urls found in base url then name the file string(rune)filenamestring etc

the output for the nuclear ajax is stored in 


"""
modules/ajax-crawlers/output
"""


